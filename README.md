# Data Lakes with Spark (In Progress)

# Overview of the project
This is to simulate a situation that a music startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to. So, this project aims to:

- Build an ETL pipeline for a data lake hosted on S3
- Load JSON data from S3
- Process the data into analytics tables using Spark (Amazon EMR Spark)
- Load data back into S3 as parquet files

## **Datasets**

### **Song Data**
Song dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). The data is in JSON format and contains metadata about a song and the artist of that song.

Sample Song Data:
```
{"num_songs":1,"artist_id":"ARD7TVE1187B99BFB1","artist_latitude":null,"artist_longitude":null,"artist_location":"California - LA","artist_name":"Casual","song_id":"SOMZWCG12A8C13C480","title":"I Didn't Mean To","duration":218.93179,"year":0}
```
### **Log Data**
Log dataset is in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

Sample Log Data:
```
{"artist":"Des'ree","auth":"Logged In","firstName":"Kaylee","gender":"F","itemInSession":1,"lastName":"Summers","length":246.30812,"level":"free","location":"Phoenix-Mesa-Scottsdale, AZ","method":"PUT","page":"NextSong","registration":1540344794796.0,"sessionId":139,"song":"You Gotta Be","status":200,"ts":1541106106796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"","userId":"8"}
```

# ETL Pipeline Process
1. Load data from S3

2. Process data using Spark

3. Load processed data back to data lake resides in S3


# Setup Configurations File at the Root Project - `dl.config`
  [AWS]
  AWS_ACCESS_KEY_ID=<iam_access_key_id>
  AWS_SECRET_ACCESS_KEY=<iam_secret_access_key>

  [S3]
  INPUT_DATA=s3a://udacity-dend/
  OUTPUT_DATA=s3://<your_s3_bucket>/data_outputs/
  
